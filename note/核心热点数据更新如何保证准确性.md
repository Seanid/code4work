核心热点数据通常指的是某些更新频率非常高的关键性统计数据，如库存，账户金额等，此类数据，在更新的时候，需要优先考虑数据的准确性，否则将会影响业务。

# 如何保证核心热点数据的准确性
1. 悲观锁：通过加锁可以保证数据的准确性，但是却会影响到性能，所以一般不推荐采用加悲观锁的方式进行处理，除非业务要求安全级别非常高
2. 基于数据库的乐观锁：在更新前先查询当前的数据信息，在更新的时候加上where xxx>={查询到的信息} 进行乐观锁的处理。这种方式，一方面可能导致更新失败，所以需要在事务内执行，保证业务的安全。另一方面，在并发高的情况下，可能会给数据库带来比较大的压力
```
此方案的注意点：
1. 需要将查询与更新的时机间隔控制的尽量小，减少出现更新失败的情况
2. 需要根据业务，设定事务的隔离级别，来实现查询到的数据的准确性。通常是RR与RC的选择，两者的关键性差别在于幻读，但是通常热点数据幻读的影响不大，可以根据业务酌情处理
3. 事务粒度要尽可能的小，从而减少数据锁的时间
2018年在做支付系统的时候，就是使用这样子的方式进行处理，在一次访问量较高的情况下，数据库的CPU和锁等待飙升，后期是通过升级数据库和降低事务粒度来临时解决。
```
3. 基于redis+Lua 的缓存解决方案：通过redis+lua可以保证数据在redis层面的准确处理，但是会出现redis 冗机导致脏数据或者数据同步延迟导致业务显示异常的情况，比较典型的就是少卖少扣
4. 后同步解决方案（2018年那个问题的备选方案）：这种方式能够保证高效与准确，却也没法解决少卖的情况
   * redis + MQ + MySQL ：通过redis 实现扣减，在通过MQ进行事件传递，最后同步到MySQL实现落盘扣减，添加MQ，可以在MySQL性能瓶颈的时候，有重试机制的可能。
   * MySQL方案 +同步：通过数据分割，将核心热点数据分割成为多条小数据来解决锁的粒度问题，然后通过同步的方式重新汇聚统计
